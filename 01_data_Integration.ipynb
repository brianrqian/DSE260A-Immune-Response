{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa67291-1319-404e-8f12-d5c8d0680092",
   "metadata": {},
   "source": [
    "# 2nd CMI-PB Prediction Challenge\n",
    "## Team Advisor: Barry Grant, Jason Hsiao\n",
    "## Team member: Peng Cheng, Javier Garcia, Brian Qian, Weikang Guan\n",
    "## Part 1: Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a214591-3c38-439e-9aa1-3b1f4c941f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary Python libraries.\n",
    "import os  # Library for interacting with the operating system\n",
    "import numpy as np  # Library for numerical operations on large arrays and matrices\n",
    "import pandas as pd  # Library for data manipulation and analysis\n",
    "from sklearn.preprocessing import OneHotEncoder  # Tool for converting categorical data into a format that can be provided to ML algorithms\n",
    "\n",
    "# Check if the directory for data does not exist, then create it.\n",
    "# This ensures that there is a 'data' directory available for storing files.\n",
    "if not os.path.exists('data'):\n",
    "    os.makedirs('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0a0d7b-b092-4271-8fd9-53554fe2c70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data into dataframes\n",
    "df_2020_specimen = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/training_data/2020LD_specimen.tsv\", sep='\\t')\n",
    "df_2020_subject = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/training_data/2020LD_subject.tsv\", sep='\\t')\n",
    "df_2020_titer = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/training_data/2020LD_plasma_ab_titer.tsv\", sep='\\t')\n",
    "df_2020_cell_freq = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/training_data/2020LD_pbmc_cell_frequency.tsv\", sep='\\t')\n",
    "df_2020_gene = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/training_data/2020LD_pbmc_gene_expression.tsv\", sep='\\t')\n",
    "#df_2020_cytokine = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/training_data/2020LD_plasma_cytokine_concentration.tsv\", sep='\\t')\n",
    "\n",
    "df_2021_specimen = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/training_data/2021LD_specimen.tsv\", sep='\\t')\n",
    "df_2021_subject = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/training_data/2021LD_subject.tsv\", sep='\\t')\n",
    "df_2021_titer = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/training_data/2021LD_plasma_ab_titer.tsv\", sep='\\t')\n",
    "df_2021_cell_freq = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/training_data/2021LD_pbmc_cell_frequency.tsv\", sep='\\t')\n",
    "df_2021_gene = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/training_data/2021LD_pbmc_gene_expression.tsv\", sep='\\t')\n",
    "#df_2021_cytokine = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/training_data/2021LD_plasma_cytokine_concentration.tsv\", sep='\\t')\n",
    "\n",
    "df_2022_specimen = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/prediction_data/2022BD_specimen.tsv\", sep='\\t')\n",
    "df_2022_subject = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/prediction_data/2022BD_subject.tsv\", sep='\\t')\n",
    "df_2022_titer = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/prediction_data/2022BD_plasma_ab_titer.tsv\", sep='\\t')\n",
    "df_2022_cell_freq = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/prediction_data/2022BD_pbmc_cell_frequency.tsv\", sep='\\t')\n",
    "df_2022_gene = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/prediction_data/2022BD_pbmc_gene_expression.tsv\", sep='\\t')\n",
    "#df_2022_cytokine = pd.read_csv(\"https://www.cmi-pb.org/downloads/cmipb_challenge_datasets/current/2nd_challenge/raw_datasets/prediction_data/2022BD_plasma_cytokine_concentration.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96b4df93-1678-4abf-8111-ffd18face36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df_subject(df):\n",
    "    \"\"\"\n",
    "    Cleans the subject DataFrame by calculating ages and applying one-hot encoding to categorical variables.\n",
    "    This function prepares the data for analysis by standardizing categorical data and simplifying the DataFrame structure,\n",
    "    making it suitable for integration into statistical models or machine learning pipelines.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The subject data as a pandas DataFrame, containing demographics and other categorical data.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The cleaned and transformed DataFrame with age calculations and encoded categorical variables.\n",
    "    \"\"\"\n",
    "    # Calculate age from date_of_boost and year_of_birth\n",
    "    df['Age'] = pd.to_numeric(df['date_of_boost'].str[:4]) - pd.to_numeric(df['year_of_birth'].str[:4])\n",
    "        \n",
    "    # Use OneHotEncoder for 'infancy_vac', 'biological_sex', 'ethnicity' and 'race' columns\n",
    "    columns_to_encode = ['infancy_vac', 'biological_sex', 'ethnicity', 'race']\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "    df_encoded = pd.DataFrame(encoder.fit_transform(df[columns_to_encode]), columns=encoder.get_feature_names_out(columns_to_encode))\n",
    "    \n",
    "    # Concatenate the encoded columns with the original DataFrame\n",
    "    result_df = pd.concat([df, df_encoded], axis=1)\n",
    "    \n",
    "    # Drop the original columns that were encoded and unnecessary columns\n",
    "    result_df.drop(columns=columns_to_encode, inplace=True)\n",
    "    result_df.drop(['year_of_birth', 'date_of_boost', \"dataset\"], axis=1, inplace=True)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def clean_df_specimen(df):\n",
    "    \"\"\"\n",
    "    Filters and cleans the specimen DataFrame by selecting specific days relative to boost and ensuring subjects have a complete set of specified days.\n",
    "    It also calculates the difference in days between planned and actual boost days for further analysis.\n",
    "    This function ensures the data is suitable for studies where consistency across multiple time points is critical.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The specimen data as a pandas DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The filtered and transformed DataFrame with additional columns for day differences and unnecessary columns removed.\n",
    "    \"\"\"\n",
    "    # Extract rows with specific planned days relative to boost\n",
    "    df = df[(df['planned_day_relative_to_boost'] == 0) | \n",
    "            (df['planned_day_relative_to_boost'] == 1) |\n",
    "            (df['planned_day_relative_to_boost'] == 3) |\n",
    "            (df['planned_day_relative_to_boost'] == 14)]\n",
    "    result_df = df[['specimen_id', 'subject_id', 'actual_day_relative_to_boost', 'planned_day_relative_to_boost']]\n",
    "    \n",
    "    # Remove subjects that do not have all specified planned days\n",
    "    result_df = result_df.groupby('subject_id').filter(lambda x: (0 in x['planned_day_relative_to_boost'].values) and\\\n",
    "                                                       (1 in x['planned_day_relative_to_boost'].values) and\\\n",
    "                                                       (3 in x['planned_day_relative_to_boost'].values) and\\\n",
    "                                                       (14 in x['planned_day_relative_to_boost'].values))\n",
    "    result_df = create_date_diff_col(result_df,'date_diff_D0',0)\n",
    "    result_df = create_date_diff_col(result_df,'date_diff_D1',1)\n",
    "    result_df = create_date_diff_col(result_df,'date_diff_D3',3)\n",
    "    result_df = create_date_diff_col(result_df,'date_diff_D14',14)\n",
    "    result_df = result_df.drop(\"actual_day_relative_to_boost\", axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def clean_df_titer(df):\n",
    "    \"\"\"\n",
    "    Transforms the titer DataFrame by restructuring it into a wide format where each row represents a specimen and \n",
    "    columns represent different isotype-antigen combinations. This format is more conducive for analyses that require \n",
    "    examining relationships or patterns across various isotype-antigen interactions within individual specimens.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The titer data as a pandas DataFrame in long format, typically with each row representing \n",
    "                    a measurement for a specific antigen and isotype for a given specimen.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The transformed DataFrame in wide format where columns are dynamically generated based on unique \n",
    "               combinations of isotype and antigen, facilitating easier access and manipulation for further analysis.\n",
    "    \"\"\"\n",
    "    # Create a new column name list based on isotype and antigen\n",
    "    new_columns = [f\"{isotype}-{antigen}\" for isotype in df['isotype'].unique() for antigen in df['antigen'].unique()]\n",
    "    \n",
    "    # Initialize an empty list to collect rows\n",
    "    rows = []\n",
    "    \n",
    "    # Iterate over the DataFrame rows\n",
    "    for index, row in df.iterrows():\n",
    "        specimen_id = row['specimen_id']\n",
    "        isotype = row['isotype']\n",
    "        antigen = row['antigen']\n",
    "        MFI_normalised = row['MFI_normalised']\n",
    "        column_name = f\"{isotype}-{antigen}\"\n",
    "        \n",
    "        # Check if there's already a row for the specimen\n",
    "        existing_row = next((r for r in rows if r['specimen_id'] == specimen_id), None)\n",
    "        if existing_row:\n",
    "            existing_row[column_name] = MFI_normalised\n",
    "        else:\n",
    "            # Create a new row with all columns initialized to zero\n",
    "            new_row = {col: 0 for col in new_columns}\n",
    "            new_row['specimen_id'] = specimen_id\n",
    "            new_row[column_name] = MFI_normalised\n",
    "            rows.append(new_row)\n",
    "    \n",
    "    # Convert list of rows to DataFrame\n",
    "    result_df = pd.DataFrame(rows)\n",
    "    result_df['specimen_id'] = result_df['specimen_id'].astype(int)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "def clean_df_cell_freq(df):\n",
    "    \"\"\"\n",
    "    Converts the cell frequency DataFrame from a long format to a wide format, where each unique cell type becomes a column header.\n",
    "    This restructuring facilitates easier manipulation and comparison of cell frequencies across different specimens,\n",
    "    making it highly suitable for analyses that require direct comparisons of multiple cell types within and across datasets.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The cell frequency data as a pandas DataFrame in long format. Each row in the DataFrame typically\n",
    "                    represents a measurement of cell frequency for a specific cell type in a specific specimen.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The transformed DataFrame in wide format with cell types as column headers and cell frequency percentages\n",
    "               as data entries, indexed by 'specimen_id'. Each row corresponds to a specimen, with cell frequencies spread across columns.\n",
    "    \"\"\"\n",
    "    result_df = df.pivot_table(index=['specimen_id'], columns=['cell_type_name'], values='percent_live_cell').reset_index()\n",
    "    return result_df\n",
    "\n",
    "def clean_df_gene(df):\n",
    "    \"\"\"\n",
    "    Converts the gene expression DataFrame from a long to a wide format, where each unique gene ID becomes a column header.\n",
    "    This restructuring facilitates easier data manipulation and analysis by aligning gene expression values (tpm) under their\n",
    "    respective gene IDs for each specimen.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The gene expression data as a pandas DataFrame in long format, where each row represents a single\n",
    "                    observation of gene expression (tpm) for a specific gene in a specific specimen.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The transformed DataFrame in wide format with gene IDs as column headers and tpm values as data entries,\n",
    "               indexed by 'specimen_id'.\n",
    "    \"\"\"\n",
    "    result_df = df.pivot_table(index=['specimen_id'], columns=['versioned_ensembl_gene_id'], values='tpm').reset_index()\n",
    "    result_df.columns = [''.join(col).strip() for col in result_df.columns.values]\n",
    "    return result_df\n",
    "\n",
    "def clean_df_cytokine(df):\n",
    "    \"\"\"\n",
    "    Converts the cytokine concentration DataFrame from a long format to a wide format. In the wide format, each unique\n",
    "    protein ID becomes a column header. This transformation is essential for facilitating easier access to and analysis of\n",
    "    cytokine concentrations across different specimens, as each row will represent a specimen with cytokine concentrations\n",
    "    laid out across columns.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The cytokine concentration data as a pandas DataFrame in long format, where each row typically\n",
    "                    represents a cytokine measurement for a specific protein in a specific specimen.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The transformed DataFrame in wide format with protein IDs as column headers and cytokine concentrations\n",
    "               (protein_expression) as data entries, indexed by 'specimen_id'.\n",
    "    \"\"\"\n",
    "    result_df = df.pivot_table(index=['specimen_id'], columns=['protein_id'], values='protein_expression').reset_index()\n",
    "    return result_df\n",
    "\n",
    "def drop_nan_col(df, cols):\n",
    "    \"\"\"\n",
    "    Removes columns with NaN (Not a Number) values from a DataFrame except for one specified column. This function is \n",
    "    particularly useful for data cleaning processes where maintaining a specific column is crucial despite its missing values.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The DataFrame from which columns will be cleaned.\n",
    "    cols (str): The column name to preserve even if it contains NaN values, ensuring it is not dropped.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The cleaned DataFrame with columns containing NaN values removed, except for the specified column.\n",
    "    \"\"\"\n",
    "    # Filter out columns not named \"Monocytes\"\n",
    "    non_monocytes_columns = [col for col in df.columns if col != cols]\n",
    "    \n",
    "    # Check if these columns contain NaN values\n",
    "    columns_with_nan = df[non_monocytes_columns].columns[df[non_monocytes_columns].isna().any()].tolist()\n",
    "    \n",
    "    # Drop columns containing NaN values\n",
    "    result_df = df.drop(columns=columns_with_nan)\n",
    "    return result_df\n",
    "\n",
    "def create_date_diff_col(df, col_name, date_num):\n",
    "    \"\"\"\n",
    "    Adds a new column to the DataFrame representing the difference in days between the planned and actual boost days for a specific date.\n",
    "    This function is particularly useful in studies where the timing of events is crucial, such as in clinical trials or time-series analyses.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The specimen data as a pandas DataFrame.\n",
    "    col_name (str): The name of the new column to create. This column will store the day differences.\n",
    "    date_num (int): The specific planned day relative to boost to calculate differences for. This helps focus the difference calculation on a particular day of interest.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The modified DataFrame with the new column added. The DataFrame is copied to avoid altering the original data.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Create a new column based on the difference between planned and actual days relative to boost\n",
    "    df_copy[col_name] = df_copy['actual_day_relative_to_boost'] - df_copy['planned_day_relative_to_boost']\n",
    "    \n",
    "    # Filter rows where 'planned_day_relative_to_boost' is date_num\n",
    "    mask = df_copy['planned_day_relative_to_boost'] == date_num\n",
    "    \n",
    "    # Update the new column to NaN where 'planned_day_relative_to_boost' is not date_num\n",
    "    df_copy.loc[~mask, col_name] = pd.NA\n",
    "\n",
    "    if col_name in df_copy.columns:\n",
    "        # Use groupby and transform to fill NaNs in the new column within each subject_id group\n",
    "        df_copy[col_name] = df_copy.groupby('subject_id')[col_name].transform(lambda x: x.ffill().bfill())\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def create_target_col(df, col_name, date_num):\n",
    "    \"\"\"\n",
    "    Creates new columns in the DataFrame that reflect target values based on specific days relative to a boost event,\n",
    "    along with fold-change calculations compared to baseline values.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): The original DataFrame containing the data.\n",
    "    col_name (str): The name of the column from which the target values are derived.\n",
    "    date_num (int): The specific day relative to the boost event to filter the DataFrame on.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: The original DataFrame with two new columns added:\n",
    "        - One column representing the absolute target values on the specified day.\n",
    "        - Another column representing the fold-change of the target values from the overall baseline to the specified day.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to get col_name values for planned_day_relative_to_boost = date_num\n",
    "    df_date = df[df['planned_day_relative_to_boost'] == date_num][['subject_id', col_name]]\n",
    "    \n",
    "    # Create a dictionary from the filtered DataFrame\n",
    "    id_to_igg = pd.Series(df_date[col_name].values, index=df_date['subject_id']).to_dict()\n",
    "    \n",
    "    # Map this dictionary to a new column in a separate DataFrame\n",
    "    new_col_name = col_name + \"_D\" + str(date_num)\n",
    "    new_fc_col_name = new_col_name + \"_FC\"\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df[new_col_name] = df['subject_id'].map(id_to_igg)\n",
    "    new_df[new_fc_col_name] = np.log2(new_df[new_col_name] / df[col_name])\n",
    "\n",
    "    # Concatenate the new DataFrame with the original DataFrame\n",
    "    df = pd.concat([df, new_df], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40fdd5b2-61df-41b9-89c0-168ba59039c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset:\n",
      "     date_diff_D0  date_diff_D1  date_diff_D3  date_diff_D14  Age  \\\n",
      "0            -3.0           0.0           0.0           -3.0   30   \n",
      "7            -3.0           0.0           0.0            0.0   33   \n",
      "11           -7.0           0.0           0.0            0.0   28   \n",
      "15           -5.0           0.0           0.0            0.0   25   \n",
      "19           -6.0           0.0           0.0            0.0   28   \n",
      "..            ...           ...           ...            ...  ...   \n",
      "360           0.0           0.0           0.0            0.0   19   \n",
      "364           0.0           0.0           0.0            0.0   23   \n",
      "368           0.0           0.0           0.0            0.0   20   \n",
      "372           0.0           0.0           0.0            0.0   21   \n",
      "376           0.0           0.0           0.0            0.0   19   \n",
      "\n",
      "     infancy_vac_wP  biological_sex_Male  ethnicity_Not Hispanic or Latino  \\\n",
      "0               1.0                  0.0                               1.0   \n",
      "7               1.0                  0.0                               0.0   \n",
      "11              1.0                  1.0                               1.0   \n",
      "15              1.0                  1.0                               1.0   \n",
      "19              1.0                  0.0                               1.0   \n",
      "..              ...                  ...                               ...   \n",
      "360             0.0                  0.0                               0.0   \n",
      "364             0.0                  0.0                               1.0   \n",
      "368             0.0                  1.0                               1.0   \n",
      "372             0.0                  0.0                               0.0   \n",
      "376             0.0                  1.0                               0.0   \n",
      "\n",
      "     race_More Than One Race  race_White  ...  ENSG00000284745.1  \\\n",
      "0                        0.0         1.0  ...                0.0   \n",
      "7                        0.0         1.0  ...                0.0   \n",
      "11                       0.0         0.0  ...                0.0   \n",
      "15                       0.0         0.0  ...                0.0   \n",
      "19                       0.0         1.0  ...                0.0   \n",
      "..                       ...         ...  ...                ...   \n",
      "360                      0.0         1.0  ...                0.0   \n",
      "364                      1.0         0.0  ...                0.0   \n",
      "368                      0.0         0.0  ...                0.0   \n",
      "372                      0.0         0.0  ...                0.0   \n",
      "376                      0.0         0.0  ...                0.0   \n",
      "\n",
      "     ENSG00000284746.1  ENSG00000284747.1  ENSG00000284748.1     task11  \\\n",
      "0                  0.0              1.173              0.000  10.874112   \n",
      "7                  0.0              1.102              0.000   7.041547   \n",
      "11                 0.0              2.047              0.000   7.896541   \n",
      "15                 0.0              0.406              0.000   5.327203   \n",
      "19                 0.0              0.721              0.000   9.128886   \n",
      "..                 ...                ...                ...        ...   \n",
      "360                0.0              1.019              0.000   3.350333   \n",
      "364                0.0              1.193              0.000   5.722838   \n",
      "368                0.0              1.247              0.336   8.813747   \n",
      "372                0.0              1.190              0.000   9.567161   \n",
      "376                0.0              0.585              0.151   3.423503   \n",
      "\n",
      "       task12     task21    task22  task31    task32  \n",
      "0    1.540948        NaN       NaN  46.410  0.239062  \n",
      "7    2.683163        NaN       NaN  26.204  0.172494  \n",
      "11   1.947941   7.211965  0.393366  13.353 -0.268462  \n",
      "15   0.488136        NaN       NaN  20.618 -0.502163  \n",
      "19   5.419294  41.380502  0.860359  19.606 -0.220520  \n",
      "..        ...        ...       ...     ...       ...  \n",
      "360  4.221438  33.000000 -0.299014  26.316  0.154588  \n",
      "364  1.527971  15.000000 -0.119909  32.281  0.455877  \n",
      "368  0.730327  39.500000  0.592286  18.308 -0.537758  \n",
      "372  1.288160  23.500000  0.441889  25.533 -0.154036  \n",
      "376  2.293249  35.000000  0.033359  22.314  0.195733  \n",
      "\n",
      "[95 rows x 58366 columns]\n",
      "\n",
      "\n",
      "Prediction dataset:\n",
      "    date_diff_D0  date_diff_D1  date_diff_D3  date_diff_D14  Age  \\\n",
      "0            0.0           0.0           0.0            0.0   35   \n",
      "4            0.0           0.0           0.0            0.0   28   \n",
      "8            0.0           0.0           0.0            0.0   22   \n",
      "12           0.0           0.0           0.0            1.0   20   \n",
      "16           0.0           0.0           0.0            0.0   18   \n",
      "20           0.0           0.0           0.0            0.0   18   \n",
      "24           0.0           0.0           0.0            0.0   27   \n",
      "28           0.0           0.0           0.0            0.0   32   \n",
      "32           0.0           0.0           0.0            0.0   27   \n",
      "36           0.0           0.0           0.0            0.0   25   \n",
      "40           0.0           0.0           0.0            0.0   23   \n",
      "44           0.0           0.0           0.0            0.0   26   \n",
      "48           0.0           0.0           0.0            0.0   32   \n",
      "52           0.0           0.0           1.0            0.0   24   \n",
      "56           0.0           0.0           0.0            0.0   25   \n",
      "60           0.0           0.0           0.0            1.0   25   \n",
      "64           0.0           0.0           0.0            0.0   31   \n",
      "68           0.0           0.0           0.0            0.0   19   \n",
      "72           0.0           0.0           0.0           -3.0   21   \n",
      "76           0.0           0.0           0.0            0.0   27   \n",
      "80           0.0           0.0           0.0            0.0   24   \n",
      "\n",
      "    infancy_vac_wP  biological_sex_Male  ethnicity_Not Hispanic or Latino  \\\n",
      "0              1.0                  1.0                               1.0   \n",
      "4              1.0                  0.0                               1.0   \n",
      "8              0.0                  0.0                               0.0   \n",
      "12             0.0                  0.0                               1.0   \n",
      "16             0.0                  1.0                               1.0   \n",
      "20             0.0                  1.0                               1.0   \n",
      "24             1.0                  0.0                               1.0   \n",
      "28             1.0                  0.0                               1.0   \n",
      "32             1.0                  0.0                               1.0   \n",
      "36             0.0                  0.0                               1.0   \n",
      "40             0.0                  0.0                               1.0   \n",
      "44             1.0                  0.0                               1.0   \n",
      "48             1.0                  0.0                               1.0   \n",
      "52             0.0                  0.0                               0.0   \n",
      "56             1.0                  1.0                               1.0   \n",
      "60             0.0                  1.0                               1.0   \n",
      "64             1.0                  1.0                               1.0   \n",
      "68             0.0                  0.0                               1.0   \n",
      "72             0.0                  1.0                               1.0   \n",
      "76             0.0                  0.0                               0.0   \n",
      "80             0.0                  1.0                               1.0   \n",
      "\n",
      "    race_More Than One Race  race_White  ...  ENSG00000284739.1  \\\n",
      "0                       0.0         1.0  ...                0.0   \n",
      "4                       0.0         1.0  ...                0.0   \n",
      "8                       0.0         0.0  ...                0.0   \n",
      "12                      0.0         1.0  ...                0.0   \n",
      "16                      0.0         1.0  ...                0.0   \n",
      "20                      0.0         1.0  ...                0.0   \n",
      "24                      0.0         1.0  ...                0.0   \n",
      "28                      0.0         0.0  ...                0.0   \n",
      "32                      0.0         1.0  ...                0.0   \n",
      "36                      0.0         1.0  ...                0.0   \n",
      "40                      0.0         0.0  ...                0.0   \n",
      "44                      0.0         1.0  ...                0.0   \n",
      "48                      0.0         1.0  ...                0.0   \n",
      "52                      0.0         1.0  ...                0.0   \n",
      "56                      0.0         1.0  ...                0.0   \n",
      "60                      0.0         1.0  ...                0.0   \n",
      "64                      0.0         0.0  ...                0.0   \n",
      "68                      0.0         0.0  ...                0.0   \n",
      "72                      0.0         1.0  ...                0.0   \n",
      "76                      1.0         0.0  ...                0.0   \n",
      "80                      0.0         0.0  ...                0.0   \n",
      "\n",
      "    ENSG00000284740.1  ENSG00000284741.1  ENSG00000284742.1  \\\n",
      "0               1.919                0.0                0.0   \n",
      "4               0.000                0.0                0.0   \n",
      "8               0.000                0.0                0.0   \n",
      "12              0.000                0.0                0.0   \n",
      "16              0.664                0.0                0.0   \n",
      "20              0.000                0.0                0.0   \n",
      "24              1.597                0.0                0.0   \n",
      "28              0.511                0.0                0.0   \n",
      "32              1.003                0.0                0.0   \n",
      "36              1.045                0.0                0.0   \n",
      "40              0.000                0.0                0.0   \n",
      "44              0.000                0.0                0.0   \n",
      "48              2.546                0.0                0.0   \n",
      "52              0.496                0.0                0.0   \n",
      "56              1.642                0.0                0.0   \n",
      "60              0.000                0.0                0.0   \n",
      "64              0.612                0.0                0.0   \n",
      "68              0.000                0.0                0.0   \n",
      "72              0.921                0.0                0.0   \n",
      "76              0.000                0.0                0.0   \n",
      "80              0.937                0.0                0.0   \n",
      "\n",
      "    ENSG00000284743.1  ENSG00000284744.1  ENSG00000284745.1  \\\n",
      "0                 0.0                0.0                0.0   \n",
      "4                 0.0                0.0                0.0   \n",
      "8                 0.0                0.0                0.0   \n",
      "12                0.0                0.0                0.0   \n",
      "16                0.0                0.0                0.0   \n",
      "20                0.0                0.0                0.0   \n",
      "24                0.0                0.0                0.0   \n",
      "28                0.0                0.0                0.0   \n",
      "32                0.0                0.0                0.0   \n",
      "36                0.0                0.0                0.0   \n",
      "40                0.0                0.0                0.0   \n",
      "44                0.0                0.0                0.0   \n",
      "48                0.0                0.0                0.0   \n",
      "52                0.0                0.0                0.0   \n",
      "56                0.0                0.0                0.0   \n",
      "60                0.0                0.0                0.0   \n",
      "64                0.0                0.0                0.0   \n",
      "68                0.0                0.0                0.0   \n",
      "72                0.0                0.0                0.0   \n",
      "76                0.0                0.0                0.0   \n",
      "80                0.0                0.0                0.0   \n",
      "\n",
      "    ENSG00000284746.1  ENSG00000284747.1  ENSG00000284748.1  \n",
      "0                 0.0              0.205               0.00  \n",
      "4                 0.0              0.074               0.00  \n",
      "8                 0.0              0.390               0.00  \n",
      "12                0.0              0.084               0.00  \n",
      "16                0.0              0.319               0.00  \n",
      "20                0.0              0.198               0.00  \n",
      "24                0.0              0.683               0.00  \n",
      "28                0.0              0.328               0.00  \n",
      "32                0.0              0.161               0.00  \n",
      "36                0.0              0.084               0.00  \n",
      "40                0.0              0.101               0.00  \n",
      "44                0.0              0.000               0.00  \n",
      "48                0.0              0.136               0.00  \n",
      "52                0.0              0.874               0.74  \n",
      "56                0.0              0.088               0.00  \n",
      "60                0.0              0.353               0.00  \n",
      "64                0.0              0.098               0.00  \n",
      "68                0.0              0.159               0.00  \n",
      "72                0.0              0.295               0.00  \n",
      "76                0.0              0.187               0.00  \n",
      "80                0.0              0.676               0.00  \n",
      "\n",
      "[21 rows x 58360 columns]\n"
     ]
    }
   ],
   "source": [
    "# Concatenate dataframes from 2020 and 2021, reset the index\n",
    "df_specimen = pd.concat([clean_df_specimen(df_2020_specimen), clean_df_specimen(df_2021_specimen)], ignore_index=True).fillna(0.0)\n",
    "df_subject = pd.concat([clean_df_subject(df_2020_subject), clean_df_subject(df_2021_subject)], ignore_index=True).fillna(0.0)\n",
    "df_titer = pd.concat([clean_df_titer(df_2020_titer), clean_df_titer(df_2021_titer)], ignore_index=True)\n",
    "df_cell_freq = pd.concat([clean_df_cell_freq(df_2020_cell_freq), clean_df_cell_freq(df_2021_cell_freq)], ignore_index=True)\n",
    "df_cell_freq = drop_nan_col(df_cell_freq, \"Monocytes\")\n",
    "df_gene = pd.concat([clean_df_gene(df_2020_gene), clean_df_gene(df_2021_gene)], ignore_index=True)\n",
    "# df_cytokine = pd.concat([clean_df_cytokine(df_2020_cytokine), clean_df_cytokine(df_2021_cytokine)], ignore_index=True)\n",
    "# df_cytokine = drop_nan_col(df_cytokine, \"P10147\")\n",
    "\n",
    "# Merge all cleaned and prepared dataframes to create a comprehensive training dataset, filling missing values with 1.0 where necessary.\n",
    "df_train = df_specimen.merge(df_subject, on='subject_id', how='left')\n",
    "df_train = df_train.merge(df_titer, on='specimen_id', how='left').fillna(1.0)\n",
    "df_train = df_train.merge(df_cell_freq, on='specimen_id', how='left')\n",
    "df_train = df_train.merge(df_gene, on='specimen_id', how='left')\n",
    "# df_train = df_train.merge(df_cytokine, on='specimen_id', how='left')\n",
    "\n",
    "# Prepare the prediction dataset in a similar fashion using 2022 data.\n",
    "df_pred = clean_df_specimen(df_2022_specimen).merge(clean_df_subject(df_2022_subject), on='subject_id', how='left')\n",
    "df_pred = df_pred.merge(clean_df_titer(df_2022_titer), on='specimen_id', how='left').fillna(1.0)\n",
    "df_pred = df_pred.merge(clean_df_cell_freq(df_2022_cell_freq), on='specimen_id', how='left')\n",
    "df_pred = df_pred.merge(clean_df_gene(df_2022_gene), on='specimen_id', how='left')\n",
    "# df_pred = df_pred.merge(clean_df_cytokine(df_2022_cytokine), on='specimen_id', how='left')\n",
    "\n",
    "# Align columns in the training and prediction datasets to ensure they are consistent.\n",
    "common_columns = df_train.columns.intersection(df_pred.columns)\n",
    "df_train = df_train.loc[:, common_columns]\n",
    "df_pred = df_pred.loc[:, common_columns]\n",
    "\n",
    "# Create target columns for the training data based on specific criteria.\n",
    "df_train = create_target_col(df_train, \"IgG-PT\", 14)\n",
    "df_train = create_target_col(df_train, \"Monocytes\", 1)\n",
    "df_train = create_target_col(df_train, \"ENSG00000277632.1\", 3)\n",
    "\n",
    "# Filter the training data to only include entries from the initial planned day and drop unnecessary columns.\n",
    "df_train = df_train[df_train['planned_day_relative_to_boost'] == 0]\n",
    "df_train = df_train.drop(['subject_id', 'specimen_id', 'planned_day_relative_to_boost', 'race_Unknown or Not Reported'], axis=1)\n",
    "\n",
    "# Rename columns for clarity in identifying tasks.\n",
    "df_train = df_train.rename(columns={\n",
    "    'IgG-PT_D14': 'task11',\n",
    "    'IgG-PT_D14_FC': 'task12',\n",
    "    'Monocytes_D1': 'task21',\n",
    "    'Monocytes_D1_FC': 'task22',\n",
    "    'ENSG00000277632.1_D3': 'task31',\n",
    "    'ENSG00000277632.1_D3_FC': 'task32'\n",
    "})\n",
    "\n",
    "# Prepare separate dataframes for each task, dropping columns not needed for specific tasks and rows with NaN values in the target column.\n",
    "df_train_task11 = df_train.drop(columns=['date_diff_D1', 'date_diff_D3', 'task12', 'task21', 'task22', 'task31', 'task32']).dropna(subset=['task11'])\n",
    "df_train_task12 = df_train.drop(columns=['date_diff_D1', 'date_diff_D3', 'task11', 'task21', 'task22', 'task31', 'task32']).dropna(subset=['task12'])\n",
    "df_train_task21 = df_train.drop(columns=['date_diff_D3', 'date_diff_D14', 'task11', 'task12', 'task22', 'task31', 'task32']).dropna(subset=['task21'])\n",
    "df_train_task22 = df_train.drop(columns=['date_diff_D3', 'date_diff_D14', 'task11', 'task12', 'task21', 'task31', 'task32']).dropna(subset=['task22'])\n",
    "df_train_task31 = df_train.drop(columns=['date_diff_D1', 'date_diff_D14', 'task11', 'task12', 'task21', 'task22', 'task32']).dropna(subset=['task31'])\n",
    "df_train_task32 = df_train.drop(columns=['date_diff_D1', 'date_diff_D14', 'task11', 'task12', 'task21', 'task22', 'task31']).dropna(subset=['task32'])\n",
    "\n",
    "df_pred = df_pred[df_pred['planned_day_relative_to_boost'] == 0].sort_values(by='subject_id')\n",
    "df_pred = df_pred.drop(['subject_id', 'specimen_id', 'planned_day_relative_to_boost', 'race_Unknown or Not Reported'], axis=1)\n",
    "\n",
    "df_pred_task11 = df_pred.drop(columns=['date_diff_D1', 'date_diff_D3'])\n",
    "df_pred_task12 = df_pred.drop(columns=['date_diff_D1', 'date_diff_D3'])\n",
    "df_pred_task21 = df_pred.drop(columns=['date_diff_D3', 'date_diff_D14'])\n",
    "df_pred_task22 = df_pred.drop(columns=['date_diff_D3', 'date_diff_D14'])\n",
    "df_pred_task31 = df_pred.drop(columns=['date_diff_D1', 'date_diff_D14'])\n",
    "df_pred_task32 = df_pred.drop(columns=['date_diff_D1', 'date_diff_D14'])\n",
    "\n",
    "# Save the final training and prediction datasets to CSV files\n",
    "df_train.to_csv(f\"data/df_train.csv\", index=False)\n",
    "df_pred.to_csv(f\"data/df_pred.csv\", index=False)\n",
    "\n",
    "# Print the final datasets for verification\n",
    "print('Training dataset:')\n",
    "print(df_train)\n",
    "print('\\n')\n",
    "print('Prediction dataset:')\n",
    "print(df_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
